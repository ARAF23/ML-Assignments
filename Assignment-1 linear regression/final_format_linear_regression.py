
'''
Assignment #1
Rezwan-Ul-Alam (ID: 2011659042)
Md. Nur Alam Jowel (ID: 2012355042)
Raian Ruku (ID: 2013409642)

'''

# -*- coding: utf-8 -*-
"""Final_format_Linear_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1effnqgEPSucPmEK9hzPMJF7LZ7D3pJQY
"""

from google.colab import drive

drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import copy

data = pd.read_excel('/content/gdrive/MyDrive/data/air.xlsx')
data.head()

"""# Data PreProcessing"""

data.info()

data['NMHC(GT)'] = data['NMHC(GT)'].astype('float64')
data.info()

df = data.drop(['Date', 'Time','T','RH'], axis=1)
df.head()

for column in df.columns:
    total = df[column].isnull().sum()
    print(f"Column '{column}' has {total} null values")
    print("\n")

check = -200
#checking the presence of -200 in each of the columns
count = {}

for col in df.columns:
    count[col] = (df[col] == check).sum()

# Display the results
for column, count in count.items():
    print(f"Column '{column}' has {check}, {count} times")

df = df.drop('NMHC(GT)', axis=1) #90% value of this feature is missing, so drop it

df.replace(-200, np.nan, inplace=True)
df.isnull().sum()

for col in df.columns:
    mean = df[col].astype(float).mean()
    df[col].replace(np.nan, mean, inplace=True)

df.describe()

df_copy = df.copy()
df1 = np.array(df_copy)
df1.shape

"""# Train Test Split"""

import math
first_75 = math.ceil(0.75*len(df1))
print(first_75)

data_train = df1[:first_75, :] #first 75% of the data
data_test = df1[first_75:, :] #last 25% of the data
print(f"Shape of training data{data_train.shape}\n")
print(f"Shape of training data{data_test.shape}")

x_train = data_train[:, :-1]  # First 10 columns
y_train = data_train[:, -1]   # Last column
x_test = data_test[:, :-1]  # First 10 columns
y_test = data_test[:, -1]   # Last column
print(x_train.shape)
print(y_train)
print(x_test.shape)
print(y_test)

"""# Cost,Grad Des, grad compute"""

def compute_cost(X, y, w, b):

    m = X.shape[0] # Here X is a metrix(not vector) with m example and n features.
    cost = 0.0

    for i in range(m):
        f_wb = np.dot(X[i], w) + b    # w is a vector in this case, not scalar. So, we use np.dot() for product
        cost = cost + (f_wb - y[i])**2
    total_cost = 1 / (2 * m) * cost

    return total_cost


def compute_gradient(X, y, w, b):
    """
    Computes the gradient for linear regression
    Args:
      X (ndarray (m,n)): m number of examples, n number of features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters
      b (scalar)       : model parameter

    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.
    """
    m = X.shape[0] # number of examples
    n = X.shape[1] # number of features
    dj_dw = np.zeros((n,)) # array of zeroes with size = n(total number of feature)
    dj_db = 0.

    for i in range(m):
        f_wb = np.dot(X[i], w) + b
        common = f_wb - y[i]
        for j in range(n):
            dj_dw[j] = dj_dw[j] + common * X[i, j]
        dj_db = dj_db + common
    dj_dw = dj_dw / m
    dj_db = dj_db / m

    return dj_db, dj_dw

def gradient_descent(X, y, w_in, b_in,  alpha, num_iters, cost_function, gradient_function):
    """
    Performs gradient descent to fit w,b. Updates w,b by taking
    num_iters gradient steps with learning rate alpha

    Args:
      X (ndarray (m,n))  : m examples, n features
      y (ndarray (m,))  : target values
      w_in (ndarray(n,)) : initial values of model parameters
      b_in (scalar): initial values of model parameters
      alpha (float):     Learning rate
      num_iters (int):   number of iterations to run gradient descent
      cost_function:     function to call to produce cost
      gradient_function: function to call to produce gradient

    Returns:
      w (scalar): Updated value of parameter after running gradient descent
      b (scalar): Updated value of parameter after running gradient descent
      J_history (List): History of cost values
      p_history (list): History of parameters [w,b]
      """

    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    w = copy.deepcopy(w_in) # avoid modifying global w_in
    p_history = []
    b = b_in
    #w = w_in

    for i in range(num_iters):
        # Calculate the gradient and update the parameters using gradient_function
        dj_db, dj_dw = gradient_function(X, y, w , b)

        # Update Parameters using equation (3) above
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion
            J_history.append( cost_function(X, y, w , b))
            p_history.append([w,b])
        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters/10) == 0:
          print(f"Iteration {i:4}: Cost {J_history[-1]:8.2f} ",
                  f"dj_dw: {dj_dw}, dj_db: {dj_db: 0.3e}  ",
                  f"w: {w}, b:{b: 0.5e}")

    return w, b, J_history,p_history #return w and J,w history for graphing

"""# 1.0e-7"""

# initialize parameters

w_init =  np.array([0,0,0,0,0,0,0,0,0])

b_init = 0.
# some gradient descent settings
iterations = 1000
tmp_alpha = 1.0e-7 #7,8,9,10 ; 9 giving a better result
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent (alpha = {tmp_alpha:.1e}): ({w_final},{b_final:8.4f})")

for i in range(5):
  print(f"alpha: {tmp_alpha:.1e}, target value: {y_test[i]:0.2f}, predicted value: {np.dot(x_test[i], w_final) + b_final:0.2f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start) : alpha ={tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(x_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(x_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(x_train[:, i], y_train, c='r', label='target(train)')
    axes[i].scatter(x_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train)')

    # Scatter plot for test set
    axes[i].scatter(x_test[:, i], y_test, c='y', label='target')
    axes[i].scatter(x_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH , alp{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# 1.0e-8"""

iterations = 1000
tmp_alpha = 1.0e-8 #7,8,9,10 ; 9 giving a better result
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent (alpha = {tmp_alpha:.1e}): ({w_final},{b_final:8.4f})")

for i in range(5):
  print(f"alpha: {tmp_alpha:.1e}, target value: {y_test[i]:0.2f}, predicted value: {np.dot(x_test[i], w_final) + b_final:0.2f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start) : alpha ={tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(x_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(x_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(x_train[:, i], y_train, c='r', label='target(training example)')
    axes[i].scatter(x_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(training example)')

    # Scatter plot for test set
    axes[i].scatter(x_test[:, i], y_test, c='y', label='target(testing example)')
    axes[i].scatter(x_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict(testing example)')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH , alpha: {tmp_alpha},iteration : {iterations}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# 1.0e-8, iteration 3000"""

iterations = 3000
tmp_alpha = 1.0e-8 #7,8,9,10 ; 9 giving a better result
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent (alpha = {tmp_alpha:.1e}): ({w_final},{b_final:8.4f})")

for i in range(5):
  print(f"alpha: {tmp_alpha:.1e}, target value: {y_test[i]:0.2f}, predicted value: {np.dot(x_test[i], w_final) + b_final:0.2f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start) : alpha ={tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(x_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(x_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(x_train[:, i], y_train, c='r', label='target(train)')
    axes[i].scatter(x_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train)')

    # Scatter plot for test set
    axes[i].scatter(x_test[:, i], y_test, c='y', label='target')
    axes[i].scatter(x_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH , alpha: {tmp_alpha},iteration:{iterations}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# 1.0e-9"""

iterations = 1000
tmp_alpha = 1.0e-9 #7,8,9,10 ; 9 giving a better result
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent (alpha = {tmp_alpha:.1e}): ({w_final},{b_final:8.4f})")

for i in range(5):
  print(f" target value: {y_test[i]:0.2f}, predicted value: {np.dot(x_test[i], w_final) + b_final:0.2f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start) : alpha ={tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(x_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(x_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(x_train[:, i], y_train, c='r', label='target value(trainexample)')
    axes[i].scatter(x_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predicted value(train example)')

    # Scatter plot for test set
    axes[i].scatter(x_test[:, i], y_test, c='y', label='target value(test example)')
    axes[i].scatter(x_test[:,i], tmp_f_wb_test, marker='x', c='g', label='prediced value(test example)')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH , alp{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# 1.0e-10"""

iterations = 1000
tmp_alpha = 1.0e-10 #7,8,9,10 ; 9 giving a better result
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent (alpha = {tmp_alpha:.1e}): ({w_final},{b_final:8.4f})")

for i in range(5):
  print(f"alpha: {tmp_alpha:.1e}, target value: {y_test[i]:0.2f}, predicted value: {np.dot(x_test[i], w_final) + b_final:0.2f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start) : alpha ={tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(x_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(x_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(x_train[:, i], y_train, c='r', label='target(train)')
    axes[i].scatter(x_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train)')

    # Scatter plot for test set
    axes[i].scatter(x_test[:, i], y_test, c='y', label='target')
    axes[i].scatter(x_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH , alp{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# 1.0e-3"""

iterations = 1000
tmp_alpha = 1.0e-3 #7,8,9,10 ; 9 giving a better result
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent (alpha = {tmp_alpha:.1e}): ({w_final},{b_final:8.4f})")

for i in range(5):
  print(f"alpha: {tmp_alpha:.1e}, target value: {y_test[i]:0.2f}, predicted value: {np.dot(x_test[i], w_final) + b_final:0.2f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start) : alpha ={tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(x_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(x_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(x_train[:, i], y_train, c='r', label='target(train)')
    axes[i].scatter(x_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train)')

    # Scatter plot for test set
    axes[i].scatter(x_test[:, i], y_test, c='y', label='target')
    axes[i].scatter(x_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH , alp{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# Feature Scaling"""

df.head()

dfs = [df]
df2 = pd.concat(dfs, ignore_index=True)

def min_max_scaling(column):
    minimum_value = column.min()
    maximum_value = column.max()
    scaled_column = (column - minimum_value) / (maximum_value - minimum_value)
    return scaled_column

# Apply min-max scaling to each numeric column in the DataFrame
all_columns = df2.select_dtypes(include=[float, int]).columns
features_columns = all_columns[:-1]
print(features_columns)
df2[features_columns] = df2[features_columns].apply(min_max_scaling)
df2.head()

df2.describe()

df_cpy = df2.copy()
df3 = np.array(df_cpy)
df3.shape

"""# train test"""

first_75_df3 = math.ceil(0.75*len(df3))
print(first_75_df3)

data_train_after_scaling = df3[:first_75_df3, :] #first 75% of the data
data_test_after_scaling = df3[first_75_df3:, :] #last 25% of the data
print(f"Shape of training data{data_train_after_scaling.shape}\n")
print(f"Shape of training data{data_test_after_scaling.shape}")

#after scaling
X_train = data_train_after_scaling[:, :-1]  # First 10 columns
Y_train = data_train_after_scaling[:, -1]   # Last column
X_test = data_test_after_scaling[:, :-1]  # First 10 columns
Y_test = data_test_after_scaling[:, -1]   # Last column
print(X_train.shape)
print(X_train)
print(X_test.shape)
print(X_test)

"""# tmp_alpha = 1.0e-3

"""

# initialize parameters

w_init =  np.array([0,0,0,0,0,0,0,0,0])

b_init = 0.
# some gradient descent settings
iterations = 1000
tmp_alpha = 1.0e-3 #10
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(X_train ,Y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f"\n alpha:{tmp_alpha}, (w,b) found by gradient descent: ({w_final},{b_final:8.4f})")

for i in range(5):
    print(f" alpha:{tmp_alpha}, target value: {Y_test[i]:0.2f}, predicted value: {np.dot(X_test[i], w_final) + b_final:0.8f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start)  alpha:{tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(X_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(X_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(X_train[:, i], Y_train, c='r', label='target(train)')
    axes[i].scatter(X_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train)')

    # Scatter plot for test set
    axes[i].scatter(X_test[:, i], Y_test, c='y', label='target')
    axes[i].scatter(X_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH  alpha:{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# tmp_alpha = 1.0e-7

"""

# some gradient descent settings
iterations = 1000
tmp_alpha = 1.0e-7 #10
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(X_train ,Y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f"\nalpha:{tmp_alpha}, (w,b) found by gradient descent: ({w_final},{b_final:8.4f})")

for i in range(5):
    print(f" alpha:{tmp_alpha}, target value: {Y_test[i]:0.2f}, predicted value: {np.dot(X_test[i], w_final) + b_final:0.8f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start) alpha:{tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(X_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(X_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(X_train[:, i], Y_train, c='r', label='target(train)')
    axes[i].scatter(X_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train)')

    # Scatter plot for test set
    axes[i].scatter(X_test[:, i], Y_test, c='y', label='target')
    axes[i].scatter(X_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH alpha:{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# tmp_alpha = 1.0e-10

"""

iterations = 1000
tmp_alpha = 1.0e-10 #10
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(X_train ,Y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f" \nalpha:{tmp_alpha}, (w,b) found by gradient descent: ({w_final},{b_final:8.4f})")

for i in range(5):
    print(f" alpha:{tmp_alpha}, target value: {Y_test[i]:0.2f}, predicted value: {np.dot(X_test[i], w_final) + b_final:0.8f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start)  alpha:{tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(X_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(X_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(X_train[:, i], Y_train, c='r', label='target(train)')
    axes[i].scatter(X_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train)')

    # Scatter plot for test set
    axes[i].scatter(X_test[:, i], Y_test, c='y', label='target')
    axes[i].scatter(X_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH  alpha:{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# 1.0e-5"""

iterations = 1000
tmp_alpha = 1.0e-5 #10
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(X_train ,Y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f" \nalpha:{tmp_alpha}, (w,b) found by gradient descent: ({w_final},{b_final:8.4f})")

for i in range(5):
    print(f" alpha:{tmp_alpha}, target value: {Y_test[i]:0.2f}, predicted value: {np.dot(X_test[i], w_final) + b_final:0.8f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start)  alpha:{tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(X_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(X_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(X_train[:, i], Y_train, c='r', label='target(train)')
    axes[i].scatter(X_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train)')

    # Scatter plot for test set
    axes[i].scatter(X_test[:, i], Y_test, c='y', label='target')
    axes[i].scatter(X_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH  alpha:{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# 1.0e-1"""

iterations = 1000
tmp_alpha = 1.0e-1 #10
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(X_train ,Y_train, w_init, b_init,tmp_alpha,
                                                    iterations,compute_cost, compute_gradient)
print(f" \nalpha:{tmp_alpha}, (w,b) found by gradient descent: ({w_final},{b_final:8.4f})")

for i in range(5):
    print(f" alpha:{tmp_alpha}, target value: {Y_test[i]:0.2f}, predicted value: {np.dot(X_test[i], w_final) + b_final:0.8f}")

# plot cost versus iteration
fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(3,3))
ax1.plot(J_hist[:iterations])
ax1.set_title(f"Cost vs. iteration(start)  alpha:{tmp_alpha}");
ax1.set_ylabel('Cost')            ;
ax1.set_xlabel(f'iteration step({iterations})')  ;
plt.show()

import matplotlib.pyplot as plt
import numpy as np

#x_train, y_train, x_test, and y_test are training and test data
tmp_f_wb_train = np.dot(X_train, w_final) + b_final #ypred
tmp_f_wb_test = np.dot(X_test, w_final) + b_final

features = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
           'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']

# Get the number of features
num_features = len(features)

# Create subplots for each feature
fig, axes = plt.subplots(nrows=(num_features + 2) // 3, ncols=3, figsize=(15, 6 * ((num_features + 2) // 3)))
axes = axes.flatten()

for i in range(num_features):

    # Scatter plot for training set
    axes[i].scatter(X_train[:, i], Y_train, c='r', label='target(training example)')
    axes[i].scatter(X_train[:,i], tmp_f_wb_train, marker='x', c='b', label='predict(train example)')

    # Scatter plot for test set
    axes[i].scatter(X_test[:, i], Y_test, c='y', label='target(test example)')
    axes[i].scatter(X_test[:,i], tmp_f_wb_test, marker='x', c='g', label='predict(test example)')

    # Set title, labels, and legend
    axes[i].set_title(f'{features[i]} vs AH  alpha:{tmp_alpha}')
    axes[i].set_ylabel('AH')
    axes[i].set_xlabel(features[i])
    axes[i].legend()

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()